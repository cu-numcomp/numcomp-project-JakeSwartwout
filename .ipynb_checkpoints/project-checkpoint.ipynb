{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: SVD Using Fixed Point Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "For this project, I was interesting in learning more about SVD. We briefly went over it's use, but I was curious how one would implement it in code. So, I found this python implementation for it:\n",
    "https://github.com/j2kun/svd\n",
    "\n",
    "While looking at this package, I found that they also used fixed point iterations, which I found to be very interesting. I'll explain how later on in this report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page specifically is the code I was looking at: https://github.com/j2kun/svd/blob/master/svd.py\n",
    "\n",
    "While it is an entire collection of files, the useful portion is a single python file named svd.py\n",
    "\n",
    "I didn't want to reupload someone else's code to github, so I put it in an alternate folder and had to phenagle to get in in here, but usually you would just do the basic\n",
    "\n",
    "```python\n",
    "from svd import svd\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "homePath = sys.path\n",
    "sys.path.append('../svd')\n",
    "from svd import svd\n",
    "sys.path = homePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the software\n",
    "\n",
    "At a high level, this software performs the \"Singular Value Decomposition\" (SVD) of a matrix. This means that it is splitting any matrix A (it doesn't even need to be square) into 3 separate matrices. We represent it like this:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "With the following properties:\n",
    "\n",
    "$U$ and $V$ have orthonormal columns\n",
    "\n",
    "$\\Sigma$ is a diagonal matrix\n",
    "\n",
    "This may seem familiar to diagonalization, as it does something similar. One of the main benefits however, is that our matrix A does not have to be square for SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the method\n",
    "\n",
    "As we've seen in class, decomposing matrices into orthonormal bases is useful in reducing the solve time.\n",
    "\n",
    "This is useful in making solving matrices easier. If we were to solve $Ax = b$, it would be useful to convert into $A = U \\Sigma V^T$. The, we can use the property of U having orthonormal columns to simplify the expression.\n",
    "\n",
    "We say that now $U\\Sigma V^Tx = B$, and applying $U^T$ to both sides allows us to remove it from the left, leaving $\\Sigma V^Tx = U^TB$. Then, if we solve this for $V^Tx$ (which would be easy, since $\\Sigma$ is diagonal), we could finish and find x by applying V to the left of our solution.\n",
    "\n",
    "So, it only takes one execution to find the values for the component matrices, and is much quicker to solve new equations once we already have the values.\n",
    "\n",
    "The specific method to get these values, as stated by the author, is \"An implementation of the greedy algorithm for SVD, using the power method for the 1-dimensional case.\" This means that we will perform the power method on our matrix A, one dimension at a time to get the final result. Dimension is a little misleading however, since it is not a column of the matrix, but rather a dimension of the transformation basis.\n",
    "\n",
    "So, the code decomposes the matrix one eigenvector at a time. To find these vectors, it performs fixed point iterations on a random vector. Since our transformation stretches all points, a random vector (after applying this transformation and normalizing it) will end up pointing more in the direction of that eigenvalue. If we do this again and again, it will move more and more towards the eigenvector with the largest eigenvalue. We are then able to remove this from our original matrix, and do it again to find the lower level dimensions. The actual algorithm is more nuanced, such as using $A^TA$ rather than just A, but the idea remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Unfortunately, I don't believe this method would be high in performance. Guessing the eigenvectors in this manner seems highly inefficient, and will force us to choose between higher accuracy and better runtime (as we choose how much we want it to converge before stopping). Beyond that, I'm sure that this type of algorithm is not well conditioned. But, this seems more of a demonstration than a full optimization, which makes sense that we wouldn't use this implementation. It was still interesting to read about nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Here, I'll show a quick example of calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.,  6., 17., 18.],\n",
       "       [ 7., 15., 14., 11.],\n",
       "       [10.,  7., 14.,  8.],\n",
       "       [10.,  4.,  6.,  2.],\n",
       "       [14., 18., 17., 15.],\n",
       "       [19., 10.,  8., 10.],\n",
       "       [14., 15., 13., 17.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[15.,  6., 17., 18.],\n",
    "     [ 7., 15., 14., 11.],\n",
    "     [10.,  7., 14.,  8.],\n",
    "     [10.,  4.,  6.,  2.],\n",
    "     [14., 18., 17., 15.],\n",
    "     [19., 10.,  8., 10.],\n",
    "     [14., 15., 13., 17.]]\n",
    "A = np.array(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged in 5 iterations!\n",
      "converged in 15 iterations!\n",
      "converged in 17 iterations!\n",
      "converged in 2 iterations!\n"
     ]
    }
   ],
   "source": [
    "sigma, u, v = svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stores the single values in a one dimensional array, but can be turned back into a square one using `np.diag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65.54751762, 11.87934328,  9.47498127,  6.37407772])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65.54751762,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , 11.87934328,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  9.47498127,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  6.37407772]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43370825, -0.30227431,  0.75891686,  0.17328245],\n",
       "       [-0.35570374,  0.56744182, -0.09446739, -0.16806248],\n",
       "       [-0.30068402, -0.03273509,  0.24353264, -0.60915394],\n",
       "       [-0.16952485, -0.35485975, -0.22784483, -0.49844102],\n",
       "       [-0.48606016,  0.3219185 , -0.21111503, -0.0876953 ],\n",
       "       [-0.35897388, -0.58494942, -0.4973935 ,  0.14462961],\n",
       "       [-0.44844813,  0.11766757, -0.10922383,  0.5418561 ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5126244 , -0.45442206, -0.52701092, -0.5029675 ],\n",
       "       [-0.7911014 ,  0.56901513,  0.21387742,  0.06809409],\n",
       "       [-0.32250827, -0.68418055,  0.48901039,  0.43445855],\n",
       "       [-0.08580918,  0.0402411 , -0.66135072,  0.74406496]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can recombine to check how well it did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.,  6., 17., 18.],\n",
       "       [ 7., 15., 14., 11.],\n",
       "       [10.,  7., 14.,  8.],\n",
       "       [10.,  4.,  6.,  2.],\n",
       "       [14., 18., 17., 15.],\n",
       "       [19., 10.,  8., 10.],\n",
       "       [14., 15., 13., 17.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recombined = u @ np.diag(sigma) @ v\n",
    "recombined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's just because it's rounding. Here is how close the values actually are to the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.55271368e-15  8.88178420e-16 -1.06581410e-14  1.06581410e-14]\n",
      " [-4.44089210e-15 -3.55271368e-15 -5.32907052e-15  8.88178420e-15]\n",
      " [-3.55271368e-15 -1.77635684e-15 -7.10542736e-15  7.10542736e-15]\n",
      " [-3.55271368e-15  0.00000000e+00 -3.55271368e-15  1.33226763e-15]\n",
      " [-3.55271368e-15  0.00000000e+00 -7.10542736e-15  1.24344979e-14]\n",
      " [-3.55271368e-15  0.00000000e+00 -5.32907052e-15  8.88178420e-15]\n",
      " [-3.55271368e-15 -3.55271368e-15 -7.10542736e-15  1.06581410e-14]]\n"
     ]
    }
   ],
   "source": [
    "diff = recombined - A\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2434497875801753e-14, -1.0658141036401503e-14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(diff), np.min(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.241851231905457e-14"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
